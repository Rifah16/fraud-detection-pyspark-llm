# Fraud Detection with PySpark and Transformers

This notebook demonstrates how to preprocess structured claim data and enrich it with LLM-based text embeddings using Hugging Face MiniLM model, followed by training a GBT classifier in PySpark.

## Sections:
1. Data Loading
2. Data Cleaning
3. Embedding Claim Notes with MiniLM
4. Feature Engineering
5. Model Training
6. Evaluation

# Load and inspect structured data
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("FraudDetectionLLM").getOrCreate()

df_claims = spark.read.option("header", True).option("inferSchema", True) \
    .csv("/mnt/data/structured_claims.csv")
df_claims.printSchema()

# Clean and preprocess (add your code here...)

# Hugging Face MiniLM Embedding
from transformers import AutoTokenizer, AutoModel
import torch

tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")

def get_embedding(text):
    tokens = tokenizer(text, return_tensors='pt', truncation=True, padding=True)
    with torch.no_grad():
        output = model(**tokens)
    return output.last_hidden_state.mean(dim=1).squeeze().tolist()

# Train GBT model using PySpark
from pyspark.ml.classification import GBTClassifier
# Pipeline and evaluation steps...
